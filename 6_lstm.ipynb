{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298718 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.08\n",
      "================================================================================\n",
      "mbtg espco zd  adksl lugkl  etop  hgwoz euey inrbgevaykmrek n ui cygfj nraerbp q\n",
      "seti u svgttssztba enrormpeaedckq st zcxloeibpneasguxbtbordwr tjkduaoejid wpieet\n",
      "pbd kba eipftuwiw whej e  td efsocfwhpf utdnmwknscameog e bmh ta  w mfiaorno jf \n",
      "elzzmreaalfmnilfyad inm yeiq  pp nfelo bbisgurdcb  elxe wxpkoco t jtptufhdtl fpm\n",
      "p lghioo osprkjr munbpwgi cofhish ekeaihc cgad  iml pm amvzampkeihrbhqx uwnun np\n",
      "================================================================================\n",
      "Validation set perplexity: 20.19\n",
      "Average loss at step 100: 2.611173 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.69\n",
      "Validation set perplexity: 10.19\n",
      "Average loss at step 200: 2.248429 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.67\n",
      "Validation set perplexity: 8.51\n",
      "Average loss at step 300: 2.101266 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.41\n",
      "Validation set perplexity: 8.03\n",
      "Average loss at step 400: 2.002164 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.70\n",
      "Validation set perplexity: 8.00\n",
      "Average loss at step 500: 1.940118 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 600: 1.910010 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 700: 1.862394 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 800: 1.815930 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 900: 1.829486 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.01\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 1000: 1.827208 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "================================================================================\n",
      "inle reapers aperiot seenmc in the as proced the ward benal chany his uncecture \n",
      "che widl brants the prodes of the tote of kinfle whhredanues porconing sevents s\n",
      "be comelitationa moteme cared htle of two nine eight feve the of of edgolobant n\n",
      "dent reseven a lice precich leving in ias of other sciction and lect of a eypley\n",
      "gre and ne two zero sovore detere is poplumed weel abreating and of with memativ\n",
      "================================================================================\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 1100: 1.774278 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 1200: 1.752276 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 1300: 1.733660 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 1400: 1.746881 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1500: 1.738447 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1600: 1.749093 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1700: 1.713655 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1800: 1.675144 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1900: 1.646489 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2000: 1.697861 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "================================================================================\n",
      "t du chizs and axternul stanfumblic latting between fure to noditation outwoms m\n",
      "ilitriguardass mediment for the studing on isome two to tote between no orthlis \n",
      "x reging to the cake two pronuction varord like to spongy united chronse and the\n",
      "king the maniza inslewove croyadly inwubre in a ben makb tone elpicate tetposing\n",
      "wid basts and machical now benis kedvic moving i diense invenctmings doining glp\n",
      "================================================================================\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 2100: 1.684699 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2200: 1.678626 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2300: 1.641183 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2400: 1.662016 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 2500: 1.683053 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 2600: 1.654755 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2700: 1.654522 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2800: 1.648466 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2900: 1.647106 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3000: 1.653445 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "================================================================================\n",
      "t from as hight were shabetive called spenden ofter in infentulisits also indebs\n",
      "fic kualh his kind inpears in quit expeart no foul days ins po cogctroding origi\n",
      "jer becams is becact online one nine eight nine infinents in the game the slonn \n",
      "d winding harame attication the uncobinting the enviry of folmsumed syd eurlano \n",
      "ited a bis manisher partanes dreach thruakences as agawight dreton allamevilatia\n",
      "================================================================================\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3100: 1.628018 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3200: 1.647460 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3300: 1.639837 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3400: 1.671361 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3500: 1.658519 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3600: 1.666687 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3700: 1.642208 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3800: 1.640134 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3900: 1.636023 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4000: 1.653850 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "================================================================================\n",
      "zer carisharosceard internaticies the gritierprances postitunal bogicatione to e\n",
      "managi durings grouknk joje flade two s orthort or we oberotial of six de idever\n",
      "qual chiedrents translation or us clamcora effhic pecthomy music ergition one ni\n",
      " rensmary bood one zero hiks and amerocies and had it he ecceds evented base qui\n",
      "agisma cirsler mirtical d one nine might the zero zero of the  she the clavil ma\n",
      "================================================================================\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4100: 1.629305 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4200: 1.634570 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4300: 1.613574 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4400: 1.610303 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 4500: 1.617362 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4600: 1.612587 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4700: 1.624974 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4800: 1.628716 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4900: 1.632178 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5000: 1.603881 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "================================================================================\n",
      "mistius by purbuls vary tabon and pong and the jung ready the large for of these\n",
      "s juzize sochiphe to one nine five zero four zero zero zero with the sgepe on th\n",
      "k althinate of the other one nine that the filld eliai prefision actrees commoxs\n",
      "haciced juses ollincelo h the commains which iq tample work on go who close jaic\n",
      "x expensive severall of the syntrote do gis to the distrabuse or kill the prence\n",
      "================================================================================\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5100: 1.602723 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5200: 1.588046 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5300: 1.578317 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5400: 1.575856 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5500: 1.564190 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5600: 1.576912 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5700: 1.565885 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5800: 1.578867 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5900: 1.572900 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6000: 1.541051 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      "eagete the confection part elitid ratheal forcers of firsm american with governe\n",
      "ration and office afous lever ooned for to trimire of the samoush coales drame m\n",
      "or eco speceating ma nine autrerph and is now a first to bank german andin is ar\n",
      "ge s brown three six one zero su or accoutles a planal deaghts the asiding s spr\n",
      "ge seetically for largy the could is devarial of a shyptthetpo for some no book \n",
      "================================================================================\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6100: 1.561863 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6200: 1.533717 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6300: 1.542507 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6400: 1.540307 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6500: 1.554129 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6600: 1.591740 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6700: 1.581696 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6800: 1.601131 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6900: 1.583398 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 7000: 1.577564 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "================================================================================\n",
      "longale adv of this word with reficienkered that regodvicus of a care laster ris\n",
      "ceraged for a clusyly man chere gaten and to that there some pebprosers decaured\n",
      "x incorcomy chimings english fishric negy elderson for det sangsy as arty of alb\n",
      "p matry a tazes he is musthics as last and may and enision subaltive decemendu a\n",
      "d and no retersing or using br d scender with jugh might the show or jusic calke\n",
      "================================================================================\n",
      "Validation set perplexity: 4.31\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # row bind of previous parameters\n",
    "  sx = tf.concat(1, [ix, fx, cx, ox])\n",
    "  sm = tf.concat(1, [im, fm, cm, om])\n",
    "  sb = tf.concat(1, [ib, fb, cb, ob])\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    s = tf.matmul(i, sx) + tf.matmul(o, sm) + sb\n",
    "    input_gate, forget_gate, update, output_gate = tf.split(1, 4, s)\n",
    "    input_gate = tf.sigmoid(input_gate)\n",
    "    forget_gate = tf.sigmoid(forget_gate)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(output_gate)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.292981 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.92\n",
      "================================================================================\n",
      "o wmtp wdhc ddeun  k n tje kmxqcere i e nxeiyimnut gqzudqcac d maee wdl l ytnelj\n",
      "  hbc  ekqcttnrrr rsdjwb og hqgsvlwqrarwbez lndjbm buoibleoihl hruhnfre  gifxeb \n",
      "bewuojlr  bk lfc aeauii d ewneppbjgg iww pzk aiipbhpjjckixk ep cruiye jaqmrcjwbv\n",
      "x dkovnpdiwtaal  op aeitcsencs ukzfi uepdo eaazucei dbfsvh eeqb frfz yvpoaakci m\n",
      "rz ougrc e e loehn mzh yaaatrhlgtjcepheiu  qfcbfrf  vr  vvq  npcnlcjhozts xeae a\n",
      "================================================================================\n",
      "Validation set perplexity: 20.05\n",
      "Average loss at step 100: 2.592578 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.79\n",
      "Validation set perplexity: 10.71\n",
      "Average loss at step 200: 2.248578 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.32\n",
      "Validation set perplexity: 8.79\n",
      "Average loss at step 300: 2.081336 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 8.02\n",
      "Average loss at step 400: 2.031193 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.78\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 500: 1.971871 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 600: 1.887718 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 700: 1.858099 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 800: 1.858224 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.16\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 900: 1.837066 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 1000: 1.839430 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "================================================================================\n",
      "cbmes imaved to zero one  signer one nine five zero eight p one tarber verki rac\n",
      "manms biech rave hishated im convand in the air firmed boon hil one atfridation \n",
      "s whe le strauntis spimply indonish on the siments fexc pround ga cod the cully \n",
      "k of obsel on velled non p accoosk dewithere a ressing use and ilafted of the ex\n",
      "wpers sacce mup coopouser smnern  iss of forn scinsital dinu frenconady monshory\n",
      "================================================================================\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 1100: 1.797533 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 1200: 1.763860 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 1300: 1.755761 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 1400: 1.757678 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1500: 1.742521 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1600: 1.725449 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1700: 1.713690 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1800: 1.687750 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 1900: 1.690155 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 2000: 1.677954 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      "pane while earled tweings in chured two deflect lons of hindd langer is consicts\n",
      "ancea mosted theuth the bus commonsar notimy the at that congrietifle of for the\n",
      "te tiventian transeda is malelionantagatio alvosid in the years in borr can meti\n",
      "ons in first the hemogra per exymatt of go refore manjerguented brough a poblar \n",
      "ring pugkally instantly guidess to dow even volul and the evice gue of evarts br\n",
      "================================================================================\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 2100: 1.682906 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2200: 1.703187 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2300: 1.707275 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2400: 1.681165 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2500: 1.688669 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2600: 1.668418 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2700: 1.678768 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2800: 1.679469 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2900: 1.673256 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 3000: 1.683363 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "================================================================================\n",
      "o deconed the oditing leantatted prain adsraway grensgured for adoub also posbra\n",
      "a nevalsing the emain agnound to cress we clase way of mostly by the gunder act \n",
      "ine to mectmence mark becousey in the used would attrectually it irovelled mbsse\n",
      "y despris of has short is the struduel ay positivel dorved where reate with tele\n",
      "get maquit for other d strater hooned curriation cavection of unto from let in t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 3100: 1.646142 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3200: 1.636515 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3300: 1.641013 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 3400: 1.629764 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3500: 1.673502 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 3600: 1.650597 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3700: 1.650965 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 3800: 1.654106 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3900: 1.648832 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4000: 1.638392 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "================================================================================\n",
      "barmed is a killing was the states alterourtucies basker arepuforically owner in\n",
      "peirular the prose a smalf shorus properexent the recricafism alterntustrole and\n",
      "zer the chament sugomy forcal eight five persional resist the islanm outself of \n",
      "etarys and to the politics the esternes the frienks one five she epfort of sepai\n",
      "duage leader wroukger of calraficitative anization and stalled germanry rusting \n",
      "================================================================================\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4100: 1.616528 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4200: 1.615180 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4300: 1.616812 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 4400: 1.608550 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4500: 1.640409 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4600: 1.618694 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4700: 1.619213 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4800: 1.603293 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4900: 1.618295 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5000: 1.611948 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "================================================================================\n",
      "plaed is activation arampt particems with wedeh than sporn nome doigual over new\n",
      "x japsed instancess againant afm grane gustent it found a woomens reputto to cul\n",
      "derands as ancelact hulk generally in unit of the space we the encommon of b nom\n",
      "tabled togeyiclanishs famity tpen operatazan region of other recentionedy forepr\n",
      "zet weapote soviet two has connews of directicy deveboral of the aquarr on salk \n",
      "================================================================================\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 5100: 1.591543 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5200: 1.590451 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5300: 1.589761 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5400: 1.589991 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5500: 1.583570 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5600: 1.556582 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5700: 1.572170 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5800: 1.593391 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5900: 1.577834 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6000: 1.578223 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "================================================================================\n",
      "quement algehterning is greating one nine nine zero eight ngu that ntwormer to m\n",
      "zer with lact of deffmician howers of cammage beakirus two zero zero zero two on\n",
      "ibhal weall the scaner collune indrvanterty and imo s solvain is a taristerm as \n",
      "le decensions of from base fried relian recedpure two zero zero tgreed of brands\n",
      "here vather is record gwern to ence religious country hymoral forps footl summas\n",
      "================================================================================\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6100: 1.573299 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6200: 1.585060 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6300: 1.581258 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6400: 1.569040 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6500: 1.551683 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6600: 1.598449 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6700: 1.567580 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6800: 1.571556 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6900: 1.567412 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 7000: 1.584110 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "ge to for hage become bianing onho units one eight in the four two three minsite\n",
      "pure edacking awain very javing inwalding overfology computer one zero zero four\n",
      "quals lesi cape wheres in cambunaust esime purchs theodes of laores erts used wh\n",
      "ged cardwasing used the united to after be but lase one nine seven six tage b we\n",
      "ke arable thereeam schasfine shopasais most for the commonly s when that often h\n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], -0.1, 0.1))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    embed = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(i, dimension=1))\n",
    "    output, state = lstm_cell(embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input_embed = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(sample_input, dimension=1))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.363864 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.90\n",
      "================================================================================\n",
      "g   e t o l f r r t q h y   r p j d f i   o l y w   h v v r e g y i s d z q v   \n",
      "q x e h u s o v a   c k o t i c a y   e m   t a e j t b q c d a t t u s v l v c \n",
      "v h t f q k d f l r t s   b b n l m   a   i p a e j o n s p i r k v f r r   a e \n",
      "m d a   a i a e e q v k d y i a v m u   n   k z t s i t a b u h r h l s m c c r \n",
      "f j p g h o f   c e o i n u r e f   h y p o   t t g     t n o r s r t r e n   c \n",
      "================================================================================\n",
      "Validation set perplexity: 1393.33\n",
      "Average loss at step 100: 2.752040 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.99\n",
      "Validation set perplexity: 12.45\n",
      "Average loss at step 200: 2.370095 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.39\n",
      "Validation set perplexity: 10.00\n",
      "Average loss at step 300: 2.160660 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.69\n",
      "Validation set perplexity: 8.80\n",
      "Average loss at step 400: 2.037836 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.14\n",
      "Validation set perplexity: 8.69\n",
      "Average loss at step 500: 2.017110 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 600: 1.922821 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 700: 1.893390 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.82\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 800: 1.859690 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.63\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 900: 1.843840 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 1000: 1.776019 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "================================================================================\n",
      "call its the provers pamertian putas neol usider usblimienter the freeurals bloo\n",
      "al there optice name a creasoty one famal ward eoin of opmages ving cain repoter\n",
      "analaged or equident the conce iring althoug na jidim remallits held s to campde\n",
      "distianly sistidents sealbable five nine eight fistansly tipapic or seeptor beit\n",
      "octuld the reas poried prinel in tymer utites the net nel hold of the recust als\n",
      "================================================================================\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 1100: 1.752668 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 6.16\n",
      "Average loss at step 1200: 1.780179 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 1300: 1.751827 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1400: 1.724013 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 1500: 1.720848 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1600: 1.710978 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1700: 1.734675 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1800: 1.703715 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1900: 1.700072 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2000: 1.716472 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "================================================================================\n",
      "show a leving seven constithus also to unwh also al the one nine nine one sijens\n",
      "x uss minst drouth lite canled asportoctartum one nine nine zero zera borya bost\n",
      "zery lex of densexary kansisted consport strodr two one eight ested relealguniou\n",
      "illdages ocerences stated to shorbed the corred allsa scently to alvd kypolity t\n",
      "lack in the ken to gooels alleged the was geot thens barn to to theselon not the\n",
      "================================================================================\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 2100: 1.701834 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 2200: 1.675217 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2300: 1.690251 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2400: 1.688557 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.26\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 2500: 1.712542 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2600: 1.678102 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2700: 1.699194 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 2800: 1.661297 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2900: 1.664450 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3000: 1.667844 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "================================================================================\n",
      "has and certs former become autolomipishil of equall of the with two five rifts \n",
      "mars europy ruture stander of the mieli idpo africhatian a sletubural cology usi\n",
      "ing in a rebute the had and aws walting its wer also is fearent this disctreds l\n",
      "by ias of junical japh alleist virks noseantationgys exterz on the throoeor arti\n",
      "comporor alvichewil of with modermal can pary memed sucaidary solose warchapal m\n",
      "================================================================================\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3100: 1.664298 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3200: 1.664664 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3300: 1.642661 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3400: 1.649412 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3500: 1.639008 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3600: 1.642973 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3700: 1.642956 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 3800: 1.643085 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3900: 1.630959 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4000: 1.640397 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "world a u all the natitial ques refens to ivoressessors dower metain arms explos\n",
      "ences gualloreds yivican in empardd janests major algoritions on hooowing have f\n",
      "nwide to his bebase of rede islaumsion to uses the large hove into germa fand of\n",
      "yficions show weter ofteng autubu insectly clossity set of in three eight ond th\n",
      "x a rebunes the ciscasssguspinetistern do the impurants or with the networking n\n",
      "================================================================================\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4100: 1.640611 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 4200: 1.630764 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 4300: 1.610555 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4400: 1.640389 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4500: 1.652178 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4600: 1.650474 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4700: 1.626084 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 4800: 1.609167 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 4900: 1.625583 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 5000: 1.650929 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.55\n",
      "================================================================================\n",
      "bult cteeccusory e exposing eweatibuspince scroear boasea to responver desceabs \n",
      "not incompraice chample and ctild books wioncely fads of the retrrionisian any b\n",
      "war derhal sasc a two zero three zero oll intenlogyer straphy the miling at cist\n",
      "ior on executring one eight paritly constitutlory nit paasmancea d three battch \n",
      "prasent is the reactin gican be the  cincealon wiskawargerized look tears acture\n",
      "================================================================================\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 5100: 1.634958 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 5200: 1.619810 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5300: 1.582470 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5400: 1.582981 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5500: 1.571569 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5600: 1.599343 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5700: 1.554564 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.91\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5800: 1.560218 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5900: 1.579635 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6000: 1.547268 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "x of betw the galdia un thems datal and released an it though chearly is with in\n",
      "grong manys englitician uponies called is a this ce a homet it location contesia\n",
      "zer guber coke for the sulties puan in norto the by cunted a tegen of mooph one \n",
      "formule microspards and alskakems generallious the degate realistss wemosite of \n",
      "kor and peope requireunds these of the rolg in chrisians kaheacy its and sult in\n",
      "================================================================================\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6100: 1.567937 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6200: 1.584804 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6300: 1.596111 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6400: 1.630290 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6500: 1.623259 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6600: 1.594689 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6700: 1.581136 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6800: 1.562762 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6900: 1.553295 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 7000: 1.567509 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "================================================================================\n",
      "rate reporting the fest which also a work the governibers providey and that is t\n",
      "wages many if hegice dran the kay as a six russicely followed after cuirbets to \n",
      "talantain p and these madas appuring times plass to glashan domist underned that\n",
      "monopf by michk was randon pecking and lines of the cheminatio chimong scoting w\n",
      "but are leabvation evica cif has p one news was the gattartizion witute teams ex\n",
      "================================================================================\n",
      "Validation set perplexity: 4.50\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(tf.truncated_normal(\n",
    "        [vocabulary_size * vocabulary_size, embedding_size], -0.1, 0.1))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "    output, state = lstm_cell(embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = [tf.placeholder(tf.float32, shape=[1, vocabulary_size]) for _ in range(2)]\n",
    "  sample_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_embed = tf.nn.embedding_lookup(vocabulary_embeddings, sample_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.337484 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.15\n",
      "================================================================================\n",
      "bo g i b y g f q y p r k q h a r l   g l j d e u e k g v   a f     m h k y   i q \n",
      "xa w i r n r z a j e p g j k i r j bes g t c e h i  ec t n t i g   v e l a e t r \n",
      "xu s r p i r   h t u a i q v c m r c i s u aei k c   l   y p w r b i re  i   bef \n",
      "tz q o p e     w h o   t r r r     i k c e e s d v n i l j r n r e f r c   p d n \n",
      "fz v c g r n   e u     q n   n o g d i o w s x y a t s u neu     f a s z l y gef \n",
      "================================================================================\n",
      "Validation set perplexity: 267.63\n",
      "Average loss at step 100: 2.754383 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.97\n",
      "Validation set perplexity: 11.32\n",
      "Average loss at step 200: 2.241277 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.77\n",
      "Validation set perplexity: 9.58\n",
      "Average loss at step 300: 2.118415 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.83\n",
      "Validation set perplexity: 8.72\n",
      "Average loss at step 400: 2.019507 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.13\n",
      "Validation set perplexity: 8.50\n",
      "Average loss at step 500: 1.958915 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 8.34\n",
      "Average loss at step 600: 1.920702 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 700: 1.893389 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 800: 1.869275 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 900: 1.836953 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 7.78\n",
      "Average loss at step 1000: 1.801914 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "================================================================================\n",
      "xforein the with is a fort al mira they wone ling down to workents aat that the y\n",
      "madatudy his the reaust upob ext over the glssolopkoadents ratil world begrafaard\n",
      "pb bractrousandardaundine nations nauntric facial sany to ambarsage pa quarcks of\n",
      "jcolong gautents ado rauch islad stors behan acces out impared bidian from exartr\n",
      "wb acluding aboutemmin one seven twone nine four have guss notion intempted inali\n",
      "================================================================================\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 1100: 1.794172 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 7.92\n",
      "Average loss at step 1200: 1.786822 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 1300: 1.776000 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 1400: 1.736375 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 1500: 1.750494 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 1600: 1.755213 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 1700: 1.700423 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 8.27\n",
      "Average loss at step 1800: 1.688507 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 1900: 1.702410 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 2000: 1.701929 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      " king phills valium from of the one nine that sever of artii buirs hild to tevens\n",
      "yfention of nolt include maty or fasine of red residerity pit his it separcles de\n",
      "zo that four two s whellist the gally the lock contaceiversic impleecial relation\n",
      "ibobeer systemment type hannel senson collerated four referevent is relature stan\n",
      "ck to famidly waims the differe sconson in text one nine s grows overates for def\n",
      "================================================================================\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 2100: 1.682241 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 2200: 1.666948 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 2300: 1.636802 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 2400: 1.639949 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 2500: 1.645524 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 2600: 1.627960 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 2700: 1.629438 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 2800: 1.595712 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 8.02\n",
      "Average loss at step 2900: 1.604038 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 3000: 1.632694 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "================================================================================\n",
      "xc a seens a more haviouselate the descrial cience membras and and is a centing t\n",
      "ully and people political sktch the necess the mount a responcle he was wo yearly\n",
      " grankal phosoph respired woolas fhit as of hessage and out protect foundersond c\n",
      "gulatividao which mears the nowle the some of recorda commy islotal filp and rema\n",
      "lway eventific or react lossional of progressynations owannet alxestinates of mar\n",
      "================================================================================\n",
      "Validation set perplexity: 7.98\n",
      "Average loss at step 3100: 1.601082 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 8.31\n",
      "Average loss at step 3200: 1.590434 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 3300: 1.600014 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 3400: 1.610973 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 3500: 1.610352 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 3600: 1.620606 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 8.02\n",
      "Average loss at step 3700: 1.598075 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 7.89\n",
      "Average loss at step 3800: 1.596518 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 3900: 1.595236 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 4000: 1.612792 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "fd the sign and unieting of the proalically zen be symes s dedellative proared as\n",
      "breatune belsd which are elabity in two zero five five doans or the five been zer\n",
      "x intopia came one six sy wakhul kra barownia explacy christ serouss lonts to sys\n",
      "ynt and of the rive seven wishaeture iltions afficristion of coleum on of brealom\n",
      "jting during hasfievers willd raph experiving three zero zero zero degreat ter li\n",
      "================================================================================\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 4100: 1.617663 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 4200: 1.596905 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 7.89\n",
      "Average loss at step 4300: 1.617711 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 7.74\n",
      "Average loss at step 4400: 1.617256 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 4500: 1.618557 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 4600: 1.616976 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 4700: 1.629612 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 4800: 1.591668 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 4900: 1.567134 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 5000: 1.576537 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "================================================================================\n",
      "dxic films screeted that the computer macrisc die to was and per carge in the r t\n",
      "bbire the cellation suggests the courtea three schoughter remocrely music sorlass\n",
      "athe west landes a peacid strack s desenalies the notioning c sports for a by lig\n",
      "ught s from the skill rike of differences forver prose hen iding a no schoe whose\n",
      "known of bac court of modern mongue tabile in those of milds in the sabgia of the\n",
      "================================================================================\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 5100: 1.570961 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 5200: 1.584334 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 5300: 1.582868 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 5400: 1.586668 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 5500: 1.574901 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 5600: 1.601891 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 5700: 1.581836 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 5800: 1.589929 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 5900: 1.595026 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 6000: 1.587792 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.19\n",
      "================================================================================\n",
      "qvh are neffection of fleet for variaz and totla he coblaced the polities incover\n",
      "bf oppos there rupacists as light one eight one six one misse neets a o playone o\n",
      "bn the run the attribute of americal ice commonorial immerate and trained by copp\n",
      "sking parts feedually related food at reducies dave anstablishiev opeuvery sectio\n",
      "tjept lumby to rebussage the lords a prevered in avioes mixion paulay patince rap\n",
      "================================================================================\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 6100: 1.573140 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 6200: 1.562229 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 6300: 1.560658 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 6400: 1.546170 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.98\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 6500: 1.562945 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 6600: 1.532832 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 6700: 1.537822 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 6800: 1.530893 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 6900: 1.523430 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 7000: 1.532005 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.12\n",
      "================================================================================\n",
      "hon environments with the tord theseam to canalitian in luggled lean use of pages\n",
      "iston by the more purn were hodern when of them per became minder as algion in th\n",
      "qaals to debean internant parliaments have century the seen of house l erchipangs\n",
      "qx its consist be just one eight six eight frin  one eight feern ewhile belue giv\n",
      " younged the make been can be meanny manianonic paraims frommunitiels william or \n",
      "================================================================================\n",
      "Validation set perplexity: 7.20\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):\n",
    "            feed.append(sample(random_distribution()))\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input[0]: feed[0], sample_input[1]: feed[1]})\n",
    "            feed.append(sample(prediction))\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input[0]: b[0], sample_input[1]: b[1]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(tf.truncated_normal(\n",
    "        [vocabulary_size * vocabulary_size, embedding_size], -0.1, 0.1))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "    drop1 = tf.nn.dropout(embed, keep_prob)\n",
    "    output, state = lstm_cell(drop1, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.nn.dropout(tf.concat(0, outputs), keep_prob), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = [tf.placeholder(tf.float32, shape=[1, vocabulary_size]) for _ in range(2)]\n",
    "  sample_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_embed = tf.nn.embedding_lookup(vocabulary_embeddings, sample_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.316702 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.57\n",
      "================================================================================\n",
      "qb o t g m p t j q f r i n p i w t r e z s s v f c o o l t   t q   c j r   xea h \n",
      "ll l r z l d v r c c i m v   x o e g a m d   n n n b x y d m r k v n e q s q e c \n",
      "cv t t z g     y a t eo  q j z n j z g k l e s h e k q a k h n t t v o y d f r o \n",
      "zf g k   e s i   s e t z u t z i f e l l e e b w i m t     d p   e w e n r e   i \n",
      "dk s m e r   v a n i k t a e p u t l e h e m a b r q   e x b o f s m h i e p m n \n",
      "================================================================================\n",
      "Validation set perplexity: 168.69\n",
      "Average loss at step 100: 2.895860 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.09\n",
      "Validation set perplexity: 12.94\n",
      "Average loss at step 200: 2.433340 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.75\n",
      "Validation set perplexity: 10.67\n",
      "Average loss at step 300: 2.291744 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.34\n",
      "Validation set perplexity: 9.49\n",
      "Average loss at step 400: 2.223738 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.62\n",
      "Validation set perplexity: 8.97\n",
      "Average loss at step 500: 2.192800 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.08\n",
      "Validation set perplexity: 8.69\n",
      "Average loss at step 600: 2.189844 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.57\n",
      "Validation set perplexity: 8.38\n",
      "Average loss at step 700: 2.152176 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.27\n",
      "Validation set perplexity: 8.23\n",
      "Average loss at step 800: 2.137341 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.85\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 900: 2.114930 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.18\n",
      "Validation set perplexity: 8.15\n",
      "Average loss at step 1000: 2.104520 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.61\n",
      "================================================================================\n",
      "kure bernrepilly famb nobure bolible bapbitoro whets furdi six a phyfolian eight \n",
      "zjaws sutled junin in eix a near lositix six sfor paritation whing stor adtent by\n",
      " comnemby is in comph ares one fimation one the score alan ard mispive seben moth\n",
      "ut of to of temlmes eight of nine seven ses yor see vian sqoicsicyss stoda an ona\n",
      " in is acleaboe upture two usel with gram the woller inte dediningus corpampriant\n",
      "================================================================================\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 1100: 2.075412 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.56\n",
      "Validation set perplexity: 8.09\n",
      "Average loss at step 1200: 2.088472 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.22\n",
      "Validation set perplexity: 8.27\n",
      "Average loss at step 1300: 2.062686 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.02\n",
      "Validation set perplexity: 7.98\n",
      "Average loss at step 1400: 2.061049 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.81\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 1500: 2.059518 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 8.00\n",
      "Average loss at step 1600: 2.062631 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.06\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 1700: 2.049912 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.61\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 1800: 2.036867 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 1900: 2.041283 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.89\n",
      "Validation set perplexity: 7.98\n",
      "Average loss at step 2000: 2.037771 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.42\n",
      "================================================================================\n",
      "eses indern cive is led suand inflospecturary overy somon nehoobqrg amemeng baste\n",
      "jves geflent le for sold the fired thes hoamt ins the hics yegod aldrymomfs the w\n",
      "lcula commacluder repoce the precan uniford bors and unlit unty pprove dovu compa\n",
      "bped gado sada one ant remisornigohnuu ease and and provers inupot ints clogate i\n",
      "hcy the of werheir le cariyanaming inly irsart leenal onation cotand repan ons ho\n",
      "================================================================================\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 2100: 2.034297 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.23\n",
      "Validation set perplexity: 7.95\n",
      "Average loss at step 2200: 2.018983 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.37\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 2300: 1.998470 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.98\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 2400: 2.024308 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.61\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 2500: 2.022808 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 2600: 2.037764 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.26\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 2700: 2.022765 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.33\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 2800: 2.011248 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 2900: 2.016416 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.06\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 3000: 2.010055 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.35\n",
      "================================================================================\n",
      "z the restuding the cons i of corting spaint halm brindow in rongs thocapbakect t\n",
      "hreart resarefe lic a rosystegiet aught of in to stonce would wormablet it protle\n",
      "wqaharto of deight are nust for cas itiglare weir yueng for abord the harroud zer\n",
      "bsuuig replipanal bly id radiany hassony suustyth the of mathfation urchowrian bo\n",
      "mmy susers bang mole of art in jorch word it four the reutist k its zero faweiry \n",
      "================================================================================\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 3100: 2.013759 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.61\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 3200: 1.999714 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.72\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 3300: 2.018704 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.00\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 3400: 2.011834 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.35\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 3500: 1.993830 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.45\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 3600: 2.004660 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.09\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 3700: 1.997214 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.90\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 3800: 2.002908 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 3900: 1.986263 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.30\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 4000: 2.004856 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.07\n",
      "================================================================================\n",
      "kkan ix febihy happicromine coltiouse gortemed apter recenhivel moseric the us wi\n",
      "bas bacy relove pric ss betuharing domenturistal sherned teless been exprided est\n",
      "dxan bees one two zero zero albarnbearth in chend one nine a nades heration behey\n",
      "zkide his folow the dem the trophe rece active nine by eupg tation ta een pentrif\n",
      "giov recameoly arnesz of mesletmal fatuy lisity mof asen one five zero zero b in \n",
      "================================================================================\n",
      "Validation set perplexity: 7.74\n",
      "Average loss at step 4100: 2.041782 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.14\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 4200: 1.984310 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 4300: 2.023502 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 4400: 2.002472 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.53\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 4500: 1.995404 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.56\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 4600: 1.980151 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.18\n",
      "Validation set perplexity: 7.82\n",
      "Average loss at step 4700: 1.993073 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.88\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 4800: 2.001781 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.39\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 4900: 1.992108 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.20\n",
      "Validation set perplexity: 7.74\n",
      "Average loss at step 5000: 2.004575 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.37\n",
      "================================================================================\n",
      "kite refase the tert monly to pasantdos in ho in gernally inticle engfunt and ite\n",
      "k acconsider rememined bidala see that cond comparsim euat mempwar unight furtien\n",
      "my mepoftated is telen e was leight as two was of carbies arion in one nine a une\n",
      "afted of of hover is howentury pout prosedetion ang chric itfa ences in or are to\n",
      "pbint set me melly to to in deman avistory to were octing and one seven fame was \n",
      "================================================================================\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 5100: 1.997797 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.87\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 5200: 1.983017 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.48\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 5300: 1.981719 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 5400: 1.957733 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.67\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 5500: 1.962458 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.18\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 5600: 1.997860 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.07\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 5700: 1.971231 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.95\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 5800: 1.967051 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.68\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 5900: 1.965785 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.87\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 6000: 1.970377 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.80\n",
      "================================================================================\n",
      "vlet heir bofive had pernally of cuse arly mustrice not defsing of wort call lufe\n",
      "yn pernotennom enlogice magcs befell centry sto govere four m neasamerttenn simel\n",
      "tvomicbc volentdioruv go sa six as gartbals the lost from ust suck nowed s shoati\n",
      "pvothe tipuly with pear of adiherxaate durning the dets to a s a the i in conto f\n",
      "ich jorte in are simpecta yeon would daconine accotivepodet a mplay with leophics\n",
      "================================================================================\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 6100: 1.979380 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.75\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 6200: 1.964355 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 6300: 1.957510 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.44\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 6400: 1.976479 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.70\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 6500: 1.959440 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.63\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 6600: 1.972154 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 6700: 1.969086 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.67\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 6800: 1.979052 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.83\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 6900: 1.945262 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.23\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 7000: 1.954780 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.40\n",
      "================================================================================\n",
      "lue ger hisher is of the souric the coneptes bides approoty mett sibutigoiling of\n",
      "ebrafed incial studity brenasynstle of the eight five four one seven nine four th\n",
      "emander bage anvioring at bed the for of in decierly and mote six vels would from\n",
      "bjected on nomitary to inva vamentivition eaniwy purval evied pare claw trectodev\n",
      "jheral rojii viunaate in for war in efirse fegificet ut etctling the ran the game\n",
      "================================================================================\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 7100: 1.975450 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.20\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 7200: 1.962548 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.15\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 7300: 1.979212 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 7400: 1.968394 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.06\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 7500: 1.966311 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.16\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 7600: 1.972860 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.05\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 7700: 1.954406 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 7800: 1.952348 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.62\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 7900: 1.934438 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 8000: 1.952206 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.13\n",
      "================================================================================\n",
      "pkays bashing a t atelue to the was recept for the teand gelmmin as mern dii ofs \n",
      "fbaton or cornce turnaminite zero che his haur oppe of as alcention demilartillev\n",
      "wfal cellived housbreight aavan clorn cerments protes sengliny allepbmy c maying \n",
      "hhen instran cendip desix of the luges grould formes with comperva name stest bus\n",
      "ince the of beill isnocinst of of one nine thics parit intibut losurole jeogaton \n",
      "================================================================================\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 8100: 1.954763 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.04\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 8200: 1.948842 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.07\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 8300: 1.977294 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.98\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 8400: 1.953363 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.39\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 8500: 1.999151 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 8600: 1.958365 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.96\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 8700: 1.947073 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.80\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 8800: 1.957146 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.19\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 8900: 1.948987 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.96\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 9000: 1.956522 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.92\n",
      "================================================================================\n",
      "fzarao their that at king portily of the gion fb the one one regaws conserger of \n",
      "nquech beswided and dueration also the monbive one by astate are to palsarinant l\n",
      "bland lei and as beight phan escredoaal ansific to perripment jof terreatia meusb\n",
      "ic ong home book rulandon was kaelop clibum s jeat who of yead cons firegner that\n",
      "parctbramberoth voled cent beconto stationa of faciolier jron to the ussarm tenso\n",
      "================================================================================\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 9100: 1.961366 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 9200: 1.924063 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.28\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 9300: 1.943016 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.58\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 9400: 1.947771 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 9500: 1.934276 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 9600: 1.939449 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.72\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 9700: 1.978264 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.97\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 9800: 1.978224 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.96\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 9900: 1.974336 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.42\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 10000: 1.948343 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.77\n",
      "================================================================================\n",
      "gther is relicic the when on gerliar foundide portance sime cact and zero seven n\n",
      "qhas dench sdoildstusiscruvers of the chemabasits agoinner ascape one nine reathe\n",
      "zvlactorasendaniane buzeld resters ford eztend were the one nine nine trixos s bu\n",
      "equestan zero seve minlia butemer high the increorution the muse diuts guispine s\n",
      "fyenned of innevels was equin theres his was apmint his nogian the of the to empi\n",
      "================================================================================\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 10100: 1.956641 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 10200: 1.959735 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.56\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 10300: 1.964259 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.26\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 10400: 1.932149 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 10500: 1.962450 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.83\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 10600: 1.944483 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.68\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 10700: 1.958175 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 10800: 1.961519 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.10\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 10900: 1.962113 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 11000: 1.973888 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.67\n",
      "================================================================================\n",
      "bhers often as defbumpand of say shoath have heir maiublishe of famonstican sence\n",
      "yvate eate erchlits or based sughts is vears for to the the suated is raed with r\n",
      "iharmates a sebed have a of chrstratality one chanvarlit seer cainflues det volt \n",
      "igion the poutter rate s curqbies constic in zine six one nine not avain a gramos\n",
      "cvala larte lew the govers to falnow the s affen rimcized fent fed were esperplei\n",
      "================================================================================\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 11100: 1.966940 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 11200: 1.947170 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.09\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 11300: 1.984600 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.12\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 11400: 1.980506 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 11500: 1.956279 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.87\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 11600: 1.951101 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 11700: 1.945390 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.17\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 11800: 1.941709 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.64\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 11900: 1.963578 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.22\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 12000: 1.970481 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.85\n",
      "================================================================================\n",
      "ag the insteopers turn in alme gates westials of sentationces and is pitan a lare\n",
      "bzation seclear parguy in demicur spire jane a seants be ento the lokein decovide\n",
      "xrents arman etreir with many the diquember kinly muntice one nine three chapimt \n",
      "cnu most that fencalian one nine five five rice led to the undays by madke sta as\n",
      "kolreyar fantwo one three capp boesity loff seibant araser u nuater rement fam of\n",
      "================================================================================\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 12100: 1.942340 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.05\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 12200: 1.965083 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 12300: 1.971332 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.14\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 12400: 1.976183 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.61\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 12500: 1.940427 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 12600: 1.965001 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 12700: 1.955399 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.07\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 12800: 1.961690 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 12900: 1.951060 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.04\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 13000: 1.966020 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.94\n",
      "================================================================================\n",
      "pk rip used reoput dotation are that eight was one eight states mough who dian fi\n",
      "year of the braolosem foliting sader of iultaril monturtion by julo mile herd whe\n",
      "dmht that the exand lat the tend heigh hol votor hor new euchines one zero mass d\n",
      "xive of parch which traischinly ats which pie dasing the songpacted of and the si\n",
      "oject the were nical notation withouks which an havy entuya one of five in the vo\n",
      "================================================================================\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 13100: 1.958640 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.24\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 13200: 1.980826 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 13300: 1.971901 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 13400: 1.982742 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.29\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 13500: 1.993696 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.44\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 13600: 1.967258 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 13700: 1.975220 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 13800: 1.980126 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.31\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 13900: 1.968807 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 14000: 1.947940 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.02\n",
      "================================================================================\n",
      "ad of garp miles of fey choock the ectrigning in and everi nungtioned notal preng\n",
      "rving piol fircdaa and of that stipcon side not of one eight seven slawn mennlent\n",
      "ppkm e consix one nine three cryusing put state managram up eger and one two milm\n",
      "gxaphil for as bantic ste faor one muscablin thysing the nolculaw to year recaop \n",
      "egress winly the beopled nine seven aughitated the the secewilo greetion to mikod\n",
      "================================================================================\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 14100: 1.963103 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 14200: 1.953736 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 14300: 1.928528 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.68\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 14400: 1.957321 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 14500: 1.951764 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 14600: 1.942944 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.28\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 14700: 1.967298 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 14800: 1.938550 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 14900: 1.962614 learning rate: 0.100000\n",
      "Minibatch perplexity: 7.31\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 15000: 1.948306 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.60\n",
      "================================================================================\n",
      "effents of two nine nine four in withs flanare ther one two for am let in for pup\n",
      "eic thokey ection a thore mystlide contedion gensale uman proeper rechaull at of \n",
      "iyorreudebifart of the achoso recosz of nine seven three three nine one two six n\n",
      "gplight his signized unigo consecupine with hichum mays the as valligieger of x s\n",
      "zwilleral s in the curlacted the ma prones act goolkes suppune conlidelly of of t\n",
      "================================================================================\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 15100: 1.957340 learning rate: 0.010000\n",
      "Minibatch perplexity: 7.39\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 15200: 1.955947 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 15300: 1.952227 learning rate: 0.010000\n",
      "Minibatch perplexity: 8.11\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 15400: 1.936901 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.79\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 15500: 1.970434 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 15600: 1.948006 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.80\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 15700: 1.938686 learning rate: 0.010000\n",
      "Minibatch perplexity: 7.05\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 15800: 1.912334 learning rate: 0.010000\n",
      "Minibatch perplexity: 7.01\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 15900: 1.952487 learning rate: 0.010000\n",
      "Minibatch perplexity: 7.19\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 16000: 1.973915 learning rate: 0.010000\n",
      "Minibatch perplexity: 7.34\n",
      "================================================================================\n",
      "dria on extyle kgreetromce brasish was contersite is be early the seconcistes bui\n",
      "ble he sermsaw as euditian west in four oro five mowed herrors of the eaviention \n",
      "ljen such domeoluvarionas and reaschadetable tabome instrite nateria a suwine nin\n",
      "jzan gems s of it the preer stae gute one nine five one nine nine nine zero the o\n",
      "dtion valres levilail the green as can alutionally sip as and on alluction intrac\n",
      "================================================================================\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 16100: 1.955144 learning rate: 0.010000\n",
      "Minibatch perplexity: 7.98\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 16200: 1.945021 learning rate: 0.010000\n",
      "Minibatch perplexity: 7.42\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 16300: 1.944991 learning rate: 0.010000\n",
      "Minibatch perplexity: 7.55\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 16400: 1.961565 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 16500: 1.978583 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 16600: 1.955945 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.75\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 16700: 1.964083 learning rate: 0.010000\n",
      "Minibatch perplexity: 7.76\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 16800: 1.967075 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 16900: 1.946524 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 17000: 1.948402 learning rate: 0.010000\n",
      "Minibatch perplexity: 7.02\n",
      "================================================================================\n",
      "iques sfetgpany comprist a polust also which from the maluctment amposess lary th\n",
      "brei stade he hered two four s sruto the disturicie canother and molarcii the ful\n",
      "ction a tegy altpinues broat one seven zero nine nine six zero nine seven banted \n",
      "zuarry flynee mulcom is of is rultic remodia from the elevad for dymb of entlary \n",
      "owen to first of ren sect a bihe is secuning twespulative generd juthis in aged w\n",
      "================================================================================\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 17100: 1.942124 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.80\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 17200: 1.943730 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 17300: 1.943410 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 17400: 1.942080 learning rate: 0.010000\n",
      "Minibatch perplexity: 7.74\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 17500: 1.951940 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 17600: 1.919093 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 17700: 1.955704 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 17800: 1.927222 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 17900: 1.938684 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 18000: 1.949541 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.20\n",
      "================================================================================\n",
      "kposer one zero zero six five in the atch that wright perbers the the b estrail p\n",
      "rqook nots of their the to youbled the and izsgan the of stur in ended will which\n",
      "oboulvotical new rater of the meyale man to baslal lish i da ritor pow follepfor \n",
      "vght that a commate califhice reigh reight in afuch greenne form cray and would o\n",
      "jsher or van withly in in one two four four one five vol one and mipaph fusbown n\n",
      "================================================================================\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 18100: 1.964121 learning rate: 0.010000\n",
      "Minibatch perplexity: 7.30\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 18200: 1.952228 learning rate: 0.010000\n",
      "Minibatch perplexity: 7.46\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 18300: 1.942546 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 18400: 1.938279 learning rate: 0.010000\n",
      "Minibatch perplexity: 7.18\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 18500: 1.955887 learning rate: 0.010000\n",
      "Minibatch perplexity: 7.75\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 18600: 1.969982 learning rate: 0.010000\n",
      "Minibatch perplexity: 7.01\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 18700: 1.963557 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 18800: 1.958373 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 18900: 1.955644 learning rate: 0.010000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 19000: 1.960995 learning rate: 0.010000\n",
      "Minibatch perplexity: 8.18\n",
      "================================================================================\n",
      "ujsosciseth dectured in ong ritions to was in in dust was the insicand kin the hr\n",
      "ise actally of wulf mekretive the of one nine seven the home from burly the i in \n",
      "sqose concenpowel lim pauilbid preverics of mileas one four a pocemphunech gonasn\n",
      "tinate gence a is exege astiter rolazing prebude but ausaw reser ligil these prib\n",
      "ge cispenar lotarly reile user of lhritations with groctions at with hoaken in is\n",
      "================================================================================\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 19100: 1.955128 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 19200: 1.916384 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 19300: 1.955783 learning rate: 0.010000\n",
      "Minibatch perplexity: 7.59\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 19400: 1.988757 learning rate: 0.010000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 19500: 1.936883 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.85\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 19600: 1.950302 learning rate: 0.010000\n",
      "Minibatch perplexity: 7.22\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 19700: 1.940027 learning rate: 0.010000\n",
      "Minibatch perplexity: 7.01\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 19800: 1.945138 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 19900: 1.943212 learning rate: 0.010000\n",
      "Minibatch perplexity: 7.19\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 20000: 1.922117 learning rate: 0.001000\n",
      "Minibatch perplexity: 6.81\n",
      "================================================================================\n",
      "dding dericze fils the t bom vererrel the sertude uxers duriya unive it dawpatbon\n",
      "ylan the keny the fugtom seven the others which ehaeor progled this sermaminal us\n",
      "fhinist is swas cotoded nevel a for to two zero geided henes whitic megroas eng f\n",
      "ne plocijents set liface zods of of of dyrabluocs in s of the pricle agence agons\n",
      "xplues the sezy crelving commert stendally tosion the stemy im been colorihe his \n",
      "================================================================================\n",
      "Validation set perplexity: 7.27\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 20001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):\n",
    "            feed.append(sample(random_distribution()))\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input[0]: feed[0], sample_input[1]: feed[1]})\n",
    "            feed.append(sample(prediction))\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input[0]: b[0], sample_input[1]: b[1]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
